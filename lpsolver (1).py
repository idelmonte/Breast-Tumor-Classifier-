# -*- coding: utf-8 -*-
"""LPsolver.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11cq8NtecZz-NIivOEW9NfJo_G7e8yUnA

## Summary
This notebook demonstrates how to use linear programming (LP) to build a linear classifier for the Breast Cancer Wisconsin (Diagnostic) dataset. We formulate the classification task as a linear programming optimization problem, introducing slack variables to handle misclassifications.
"""

# import
!pip install ucimlrepo

import pandas as pd
import numpy as np
from ucimlrepo import fetch_ucirepo
from scipy.optimize import linprog
from sklearn.preprocessing import StandardScaler

# fetch dataset
data_raw = fetch_ucirepo(id=17)

# data (as pandas dataframes)
X_raw = data_raw.data.features
y_raw = data_raw.data.targets

# # metadata
# print(data_raw.metadata)

# # variable information
# print(data_raw.variables)

"""## Dataset Overview

We use the Breast Cancer dataset from the UCI repository. The dataset includes various measurements from digitized images of breast masses. Each instance is labeled as either benign or malignant. We begin by printing the feature names and previewing the first few rows of data.
"""

print("Features:", X_raw.columns.tolist())
print("Target:", y_raw.columns.tolist())

print(X_raw.head())
print(y_raw.head())

"""## Feature Selection and Preprocessing

From the complete feature set, we select five features that are typically useful in classification tasks in this field: radius (mean), perimeter (mean), area (mean), concavity (mean), and concave points (mean). These are standardized to zero mean and the target variable is also encoded: malignant (M) as +1 and benign (B) as -1.
"""

# Select features
features = ['radius1', 'perimeter1', 'area1', 'concavity1', 'concave_points1']
X = X_raw[features].values

# Standardize selected features
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Encode target: Malignant (M) = 1, Benign (B) = -1
y = y_raw['Diagnosis'].map({'M': 1, 'B': -1}).values

"""## Linear Programming Formulation

To build a linear classifier, we formulate the task as a linear programming problem.
"""

# Add bias
X_with_bias = np.hstack([X_standardized, np.ones((X_standardized.shape[0], 1))]) # [x1, ..., x5, b]
n_samples, n_features_plus_bias = X_with_bias.shape
# n_samples = 100  # Limit to 100 samples for the LP
# print(n_samples)

# Construct inequality constraints for LP
# Variables: [w1, ..., w5, b, ξ1, ..., ξn]
A_ub = []
b_ub = []

for i in range(n_samples):
    xi = X_with_bias[i]
    yi = y[i]
    constraint = [-yi * xij for xij in xi]
    slack = [0] * n_samples
    slack[i] = -1
    row = constraint + slack
    A_ub.append(row)
    b_ub.append(-1)

"""## Slack Variable Constraints

Each slack variable ξᵢ represents how much a point violates the margin. To ensure they are non-negative (as required in the LP), we include additional constraints ξᵢ ≥ 0. These constraints are optional in `scipy.optimize.linprog` if we set appropriate bounds, but we include them explicitly for clarity.
"""

# Add slack variable constraints (ξ_i ≥ 0)
# Note: Already handled via bounds in linprog so optional
for i in range(n_samples):
    row = [0] * (n_features_plus_bias + n_samples)
    row[n_features_plus_bias + i] = -1
    A_ub.append(row)
    b_ub.append(0)

"""## Objective Function

Our goal is to find the optimal hyperplane (defined by weights `w` and bias `b`) that separates the two classes while minimizing the sum of slack variables `ξᵢ`.
Since the decision variables include both the weights/bias and slack variables, we set the cost vector `c` such that only the slack variables contribute to the objective.
"""

# Objective function
c = [0] * n_features_plus_bias + [1] * n_samples

# Convert to numpy arrays
A_ub = np.array(A_ub)
b_ub = np.array(b_ub)
c = np.array(c)

# Display shapes to confirm setup
print("Shapes of the matrices:")
print("A_ub:", A_ub.shape)
print("b_ub:", b_ub.shape)
print("c:", c.shape)

"""## Solving the Linear Program

We use `scipy.optimize.linprog` with the 'highs' method to solve the LP. We define bounds so that weights and bias are unbounded (can be negative), while slack variables are constrained to be ≥ 0.

After solving, we extract the optimal weights and bias, along with the slack variables. The objective value gives the total penalty from misclassified or margin-violating samples.
"""

# Define bounds for linprog: weights and bias can be negative, slack variables must be non-negative
bounds = [(None, None)] * n_features_plus_bias + [(0, None)] * n_samples

# Solve the LP
result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')

# Extract solution
if result.success:
    w = result.x[:n_features_plus_bias - 1]
    b = result.x[n_features_plus_bias - 1]
    slack = result.x[n_features_plus_bias:]
else:
    w = b = slack = None

# Output
print("Optimal weights:", w)
print("Optimal bias:", b)
print("Objective value (sum of slack variables):", result.fun if result.success else "Failed")
print("Number of slack variables greater than 1:", np.sum(slack >= 1))
print("Slack variables:", slack)